{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported the necessary classes\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from typing import Optional, Tuple\n",
    "from torch import Tensor\n",
    "from torch.nn.init import constant_, xavier_normal_, xavier_uniform_\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torchvision.models import vision_transformer\n",
    "from torch.nn.modules import activation\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Get pretrained weights for ViT-Base-16 (can use any ViT model)\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # weights which will be tempered\n",
    "pretrained_vit_weights_org = pretrained_vit_weights # original weights which will not be tempered\n",
    "\n",
    "# Setup a ViT model instance with pretrained weights\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "pretrained_vit_org = torchvision.models.vit_b_16(weights=pretrained_vit_weights_org).to(device)\n",
    "\n",
    "# Freeze the base parameters\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "for parameter in pretrained_vit_org.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "# Function to find the accuracy loss\n",
    "def findAccuracyLoss(\n",
    "    \n",
    "    # Takes model, encoders to be tempered, no of decimal place, path to image dataset as input\n",
    "    model: torch.nn.Module,\n",
    "    model_org : torch.nn.Module,\n",
    "    encoders : List[int],\n",
    "    decimals : int,\n",
    "    root: str,\n",
    "    image_size: Tuple[int, int] = (224, 224),\n",
    "    transform: torchvision.transforms = None,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "\n",
    "    # Create transformation for image (if one doesn't exist)\n",
    "    if transform is not None:\n",
    "        image_transform = transform\n",
    "    else:\n",
    "        image_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    model.to(device)\n",
    "    model_org.to(device)\n",
    "\n",
    "    # Turn on model evaluation mode and inference mode\n",
    "    model_org.eval()\n",
    "    model.eval()\n",
    "    \n",
    "  \n",
    "    accuracy_loss = 0 # Defined average accuracy loss\n",
    "    with torch.inference_mode():\n",
    "        for i in range(1):\n",
    "            for img in os.listdir(root):\n",
    "                img_path =  os.path.join(root , img)\n",
    "                imge = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "                # Prediction of original model without tempering weights\n",
    "                transformed_image = image_transform(imge).unsqueeze(dim=0)\n",
    "                target_image_pred = model_org(transformed_image.to(device)) \n",
    "                \n",
    "                # Manually calling the functions as we want to access the weights in between to temper it\n",
    "                out = model._process_input(transformed_image.to(device))\n",
    "                n = out.shape[0]\n",
    "                batch_class_token = model.class_token.expand(n, -1, -1) # Expand the class token to the full batch\n",
    "                out = torch.cat([batch_class_token, out], dim=1)\n",
    "                input = out\n",
    "                torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "                input = input + model.encoder.pos_embedding\n",
    "                input = model.encoder.dropout(input)\n",
    "                \n",
    "                # For all the 12 encoders of ViT-Base-16\n",
    "                for i in range(12): \n",
    "                    torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "                    encoder_block = model.encoder.layers[i]\n",
    "                    x = encoder_block.ln_1(input)\n",
    "                    \n",
    "                    # Multi head attention layer\n",
    "                    mha = encoder_block.self_attention \n",
    "                    if(i in encoders):\n",
    "                        x = torch.round(x , decimals = decimals)\n",
    "                        mha.in_proj_weight.data = torch.round(mha.in_proj_weight.data , decimals = decimals) # Rounding off weights to a particular decimal place\n",
    "                    x, _ = encoder_block.self_attention(x, x, x, need_weights=False)\n",
    "                    x = encoder_block.dropout(x)\n",
    "                    x = x + input\n",
    "                    y = encoder_block.ln_2(x)\n",
    "                    \n",
    "                    # In MLP block, only rounding off the linear layers\n",
    "                    if(i in encoders):\n",
    "                        for m in encoder_block.mlp.modules():\n",
    "                            if isinstance(m, nn.Linear): \n",
    "                                m.weight.data = torch.round(m.weight.data , decimals = decimals) # Rounding off weights to a particular decimal place\n",
    "                        y = torch.round(y , decimals = decimals)\n",
    "                        \n",
    "                    y = encoder_block.mlp(y)\n",
    "                    input = x + y \n",
    "   \n",
    "                # Executing rest of layers as it is\n",
    "                final_out = model.encoder.ln(input)\n",
    "                final_out = final_out[:, 0]\n",
    "                final_out = model.heads(final_out) # Final output\n",
    "                \n",
    "                target_image_pred_probs = torch.softmax(target_image_pred, dim=1) # Original accuracy of model\n",
    "                target_image_pred_probs_approx = torch.softmax(final_out, dim=1) # Accuracy of model with tempered weights\n",
    "\n",
    "                # Convert prediction probabilities to prediction labels\n",
    "                target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "                target_image_pred_label_approx = torch.argmax(target_image_pred_probs_approx, dim=1)\n",
    "            \n",
    "                # Total accuracy loss, considering accuracy loss of each image\n",
    "                accuracy_loss += (abs(target_image_pred_probs.max() - target_image_pred_probs_approx.max())*100)\n",
    "                \n",
    "    # Average accuracy loss, dividing by the no of images in dataset\n",
    "    print(\"Average accuracy loss : \" , accuracy_loss/100000)\n",
    "    \n",
    "encoders = [0,1,2,3,4,5,6,7,8,9,10,11] # Encoders to be tempered\n",
    "\n",
    "findAccuracyLoss(model=pretrained_vit, # Model (To be tempered)\n",
    "                   model_org = pretrained_vit_org, # Original model (Not to be tempered)\n",
    "                    root = '/home/palashdas/ILSVRC/Data/CLS-LOC/test', # Path of image dataset\n",
    "                    encoders = encoders,\n",
    "                    decimals = 2) # Set the decimal place to be rounded off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
