{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3261ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported the necessary classes\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from typing import Optional, Tuple\n",
    "from torch import Tensor\n",
    "from torch.nn.init import constant_, xavier_normal_, xavier_uniform_\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torchvision.models import vision_transformer\n",
    "from torch.nn.modules import activation\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Get pretrained weights for ViT-Base\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "\n",
    "# Setup a ViT model instance with pretrained weights\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# Freeze the base parameters\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "# Function to find the percentage hits\n",
    "# Takes two matricies, no of row zones, no of column zones, row population, column population and decimal place as input\n",
    "def give_percentage(query, t_in_proj_weights, row_zones, col_zones, row_population, col_population, decimals):\n",
    "    colx = query.shape[2]\n",
    "    rowx = query.shape[1]\n",
    "    B = query.shape[0]\n",
    "    rowy = t_in_proj_weights.shape[0]\n",
    "    coly = t_in_proj_weights.shape[1]\n",
    "    no_pairs = row_zones*col_zones*colx*row_population*col_population # Total no of pairs\n",
    "\n",
    "    pairs = torch.zeros(size=(no_pairs , 2))\n",
    "    \n",
    "    row_stride = rowx // row_zones\n",
    "    col_stride = coly // col_zones\n",
    "    row = []\n",
    "    col = []\n",
    "    \n",
    "    # Randomly choosing pairs from each row zone and column zone\n",
    "    for i in range(row_zones):\n",
    "        row.extend(random.sample(range(i*row_stride , (i + 1)*row_stride), row_population))\n",
    "    for i in range(col_zones):\n",
    "        col.extend(random.sample(range(i*col_stride , (i + 1)*col_stride), col_population))\n",
    "    curr = 0\n",
    "    for i in range(B):\n",
    "        for j in row:\n",
    "            for k in range(colx):\n",
    "                for l in col:\n",
    "                    pairs[curr][0] = query[i][j][k]\n",
    "                    pairs[curr][1] = t_in_proj_weights[k][l]\n",
    "                    curr += 1\n",
    "                    if(curr == no_pairs):\n",
    "                        break\n",
    "                if(curr == no_pairs):\n",
    "                    break\n",
    "            if(curr == no_pairs):\n",
    "                break\n",
    "        if(curr == no_pairs):\n",
    "              break\n",
    "    \n",
    "    pairs = torch.abs(pairs) # Taking mod of weights as sign doesnâ€™t matter\n",
    "    pairs = pairs.cpu().numpy() # converting the matrix into numpy array for easy handling\n",
    "    \n",
    "    setp = set() # Set of pairs string\n",
    "    unique = set()  # set of unique string\n",
    "    \n",
    "    # Converting pairs into string and then adding them into a set to remove the same pairs\n",
    "    # Reversing the second number and concatenating it with the first one\n",
    "    for i in range(pairs.shape[0]):\n",
    "        first = f'{pairs[i][0]}'\n",
    "        second = f'{pairs[i][1]}'\n",
    "        final = first + second[::-1]\n",
    "        setp.add(final)\n",
    "    \n",
    "    # Adding the reversed string again into the set after removing it once to remove the same but unordered pairs\n",
    "    while len(setp) > 0:\n",
    "        x = random.sample(setp , 1)\n",
    "        curr = x[0]\n",
    "        setp.remove(curr)\n",
    "        rev = curr[::-1]\n",
    "        setp.add(rev)\n",
    "        setp.remove(rev)\n",
    "        unique.add(str(curr))\n",
    "    return (1 - len(unique)/no_pairs)*100 # Returns the percentage of pairs removed\n",
    "\n",
    "\n",
    "# Main function to be called by user for finding percentage hits\n",
    "# Takes model, encoders, decimals upto which round off is required, path of image as input\n",
    "def find_perc_and_prob(\n",
    "    model: torch.nn.Module,\n",
    "    encoders : List[int],\n",
    "    decimals : int,\n",
    "    image_path: str,\n",
    "    image_size: Tuple[int, int] = (224, 224),\n",
    "    transform: torchvision.transforms = None,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "    # Opens image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Create transformation for image (if one doesn't exist)\n",
    "    if transform is not None:\n",
    "        image_transform = transform\n",
    "    else:\n",
    "        image_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Make sure the model is on the target device\n",
    "    model.to(device)\n",
    "\n",
    "    # Turn on model evaluation mode and inference mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        # Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "        transformed_image = image_transform(img).unsqueeze(dim=0)\n",
    "        sample_image  = transformed_image.to(device)\n",
    "        target_image_pred = model(transformed_image.to(device))\n",
    "        final_out = model(sample_image)\n",
    "        \n",
    "        # Calling the functions manually as we want to temper the data in between\n",
    "        out = model._process_input(sample_image)\n",
    "        n = out.shape[0]\n",
    "        batch_class_token = model.class_token.expand(n, -1, -1) # Expand the class token to the full batch\n",
    "        out = torch.cat([batch_class_token, out], dim=1)\n",
    "        input = out\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        input = input + model.encoder.pos_embedding\n",
    "        input = model.encoder.dropout(input)\n",
    "        \n",
    "        # For all the 12 encoders of ViT Base-16\n",
    "        for i in range(12):\n",
    "            percentage_avg = 0 # Setting percentage hits to 0\n",
    "            torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "            encoder_block = model.encoder.layers[i]\n",
    "            x = encoder_block.ln_1(input)\n",
    "            \n",
    "            # Multi head attention layer\n",
    "            mha = encoder_block.self_attention\n",
    "            if(i in encoders):\n",
    "                x = torch.round(x , decimals = decimals)\n",
    "                mha.in_proj_weight.data = torch.round(mha.in_proj_weight.data , decimals = decimals) # Rounding off weights to a particular decimal place\n",
    "                \n",
    "            # Finding the percentage hits in MHA layer for given values of row zones, column zones, row population and column population\n",
    "            percentage_avg = give_percentage(x , mha.in_proj_weight.t() , 4 , 4 , 10 , 10 ,  decimals = decimals)\n",
    "            x, _ = encoder_block.self_attention(x, x, x, need_weights=False)\n",
    "            x = encoder_block.dropout(x)\n",
    "            x = x + input\n",
    "            y = encoder_block.ln_2(x)\n",
    "            if(i in encoders):\n",
    "                cnt = 0\n",
    "                \n",
    "                # For linear layers of MLP block\n",
    "                for m in encoder_block.mlp.modules():\n",
    "                    if isinstance(m, nn.Linear):\n",
    "                        cnt = cnt + 1\n",
    "                        m.weight.data = torch.round(m.weight.data , decimals = decimals) # Rounding off weights to a particular decimal place\n",
    "                        y = torch.round(y , decimals = decimals)\n",
    "                        \n",
    "                        # Finding the percentage hits in linear layers of MLP block\n",
    "                        if (cnt == 1):\n",
    "                            percentage_avg  = percentage_avg + give_percentage( y , m.weight.t() , 4 , 4 , 10 , 10 ,  decimals = decimals)\n",
    "                        else:\n",
    "                            percentage_avg  = percentage_avg + give_percentage( y , m.weight , 4 , 4 , 10 , 10 ,  decimals = decimals)\n",
    "                            \n",
    "            # Printing the percentage hits of every encoder             \n",
    "            print(\"percentage of \",i,\"th encoder : \" , percentage_avg/3 , \"\\n\")   \n",
    "            \n",
    "            y = encoder_block.mlp(y)\n",
    "            input = x + y \n",
    "   \n",
    "        #Execute rest of layers as it is\n",
    "        final_out = model.encoder.ln(input)\n",
    "        final_out = final_out[:, 0]\n",
    "        final_out = model.heads(final_out)\n",
    "        print(\"Final output:: \")\n",
    "\n",
    "    # Converts logits to prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "    target_image_pred_probs_approx = torch.softmax(final_out, dim=1)\n",
    "\n",
    "    # Convert prediction probabilities to prediction labels\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "    target_image_pred_label_approx = torch.argmax(target_image_pred_probs_approx, dim=1)\n",
    "    \n",
    "    # Plot image with predicted label and probability\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(\n",
    "        f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max():.3f}\"\n",
    "        f\"Pred: {target_image_pred_label_approx} | Prob: {target_image_pred_probs_approx.max():.3f}\"\n",
    "    )\n",
    "    plt.axis(False)\n",
    "\n",
    "encoders = [0,1,2,3,4,5,6,7,8,9,10,11] # Encoders to be tempered\n",
    "\n",
    "find_perc_and_prob(model=pretrained_vit, # Model\n",
    "                    image_path = \"./data.JPEG\", # Path to the image\n",
    "                    encoders = encoders, \n",
    "                    decimals = 2) # Set the decimal place to be rounded off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
